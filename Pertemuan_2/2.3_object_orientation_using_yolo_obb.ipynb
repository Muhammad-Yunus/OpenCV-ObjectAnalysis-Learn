{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Detect Object Orientation using YOLO OBB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Intro To YOLO Model\n",
    "\n",
    "- Yolo (You Only Look Once) is single stage object detection model that makes predictions of bounding boxes and class probabilities all at once with a realtime processing speed.\n",
    "- Historical timeline Yolo Development : <br>\n",
    "<img src=\"resource/yolo-timeline.jpg\" width=\"800px\"><br><br>\n",
    "- `Yolo` Conceptual Design :\n",
    "    - General design Yolo architecture :<br>\n",
    "    <img src=\"resource/yolo-archi-general.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 What is YOLOv8?\n",
    "- <font color=\"orange\">YOLOv8</font> is a new state-of-the-art computer vision model built by [Ultralytics](https://www.ultralytics.com/), the creators of <font color=\"orange\">YOLOv5</font>. \n",
    "- The <font color=\"orange\">YOLOv8</font> model contains out-of-the-box support for <font color=\"orange\">object detection</font>, <font color=\"orange\">classification</font>, and <font color=\"orange\">segmentation tasks</font>, accessible through a Python package as well as a command line interface.<br>\n",
    "<img src=\"resource/YOLOv8_YouTube_Thumbnail-p-500.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=\"orange\">Yolo Model</font> can be downloaded from <font color=\"orange\">Ultralytics Assets</font> repository : https://github.com/ultralytics/assets/releases/\n",
    "- All model released in [Pytorch](https://pytorch.org/) Model format (`.pt`)\n",
    "- For example if we want to use Yolo v11, there is several naming convention like the following,\n",
    "- Based on Model Size :\n",
    "    - `yolov8n.pt` : *Nano*,\n",
    "    - `yolov8s.pt` : *Small*,\n",
    "    - `yolov8m.pt` : *Medium*,\n",
    "    - `yolov8b.pt` : *Balanced*,\n",
    "    - `yolov8l.pt` : *Large*,\n",
    "    - `yolov8x.pt` : *Extra Large*,\n",
    "- Additional model with special capability :\n",
    "    - `yolov8l-cls.pt` : *Classification Model*\n",
    "    - `yolov8l-human.pt` : *Human Detection Model*\n",
    "    - `yolov8l-obb.pt` : <font color=\"orange\">*Oriented Bounding Box Object Detection*</font>\n",
    "    - `yolov8l-pose.pt` : *Pose Estimation Model*\n",
    "    - `yolov8l-seg.pt` : *Segmentation Model*<br>\n",
    "    <img src=\"resource/ultralytics.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Yolo OBB Model\n",
    "<img src=\"resource/ships-detection-using-obb.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"OpenCV version: \" + cv2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXAMPLE 1 - Using Ultralytics Yolo OBB Model to detect Object Orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model\n",
    "model = YOLO(\"yolov8l-obb.pt\")  # load an official model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image\n",
    "image_path = \"plane.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Predict with the model\n",
    "results = model(image_path)  # predict on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw bounding boxes and orientations\n",
    "output = image.copy()\n",
    "for result in results[0]:\n",
    "\n",
    "    xywhr = result.obb.xywhr.cpu().numpy().squeeze()  # Extract [center-x, center-y, width, height, angle] from the result\n",
    "    \n",
    "    center = tuple(xywhr[:2].astype(int))  # Extract center (x, y)\n",
    "    \n",
    "    # Extract rotation angle in radians, \n",
    "    # considering the object is taller than it is wide, \n",
    "    # otherwise, we need to add np.pi to the angle\n",
    "    if(xywhr[2] < xywhr[3] ):\n",
    "        angle_rad = xywhr[4] \n",
    "    else:\n",
    "        angle_rad = np.pi - xywhr[4] # flip the angle\n",
    "    \n",
    "    # Define the length of the orientation lines\n",
    "    length = 50 \n",
    "\n",
    "    # Draw original horizontal reference line (black)\n",
    "    cv2.line(output, center, (center[0] + length, center[1]), (0, 0, 0), 2)\n",
    "    \n",
    "    # Compute and draw rotation axis (red line)\n",
    "    end_x = int(center[0] + length * np.cos(angle_rad))\n",
    "    end_y = int(center[1] - length * np.sin(angle_rad))\n",
    "    cv2.line(output, center, (end_x, end_y), (0, 0, 255), 2)\n",
    "    \n",
    "    # Draw angle text\n",
    "    angle = np.rad2deg(angle_rad)\n",
    "    text_position = (center[0] - 7, center[1] - 7)\n",
    "    cv2.putText(output, f\"{int(angle)}\", text_position, cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.6, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "# Show results using Matplotlib\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.imshow(output[:, :, ::-1])  # Convert BGR to RGB\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Object Orientation with Reference Line\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Pros ✅**\n",
    "    - ✔ **Robust to Noise & Partial Occlusion** – Learns object features beyond shape.  \n",
    "    - ✔ **Works with Any Object Type** – Can detect orientation even for irregular shapes.  \n",
    "    - ✔ **Combines Object Detection & Orientation** – Provides classification along with orientation.  \n",
    "    - ✔ **Performs Well in Real-World Scenarios** – Ideal for aerial images, robotics, and autonomous systems.  \n",
    "\n",
    "- **Cons ❌**\n",
    "    - ✖ **Requires Training Data & Annotations** – Needs a labeled dataset with object orientations.  \n",
    "    - ✖ **Computationally Expensive** – Requires a GPU for real-time inference.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source : \n",
    "\n",
    "- https://docs.ultralytics.com/tasks/obb/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BelajarOpenCV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
